{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors and Topics for Fun\n",
    "### Document classification with Gensim\n",
    "\n",
    "In this tutorial we'll classify movie plots by genre using word embeddings techniques in [gensim](http://radimrehurek.com/gensim/) . \n",
    "\n",
    "See accompanying slides in this repo.\n",
    "\n",
    "We will show how to get a __'hello-world'__ first untuned run using 7 techniques:\n",
    "\n",
    "- Bag of words\n",
    "\n",
    "- Character n-grams\n",
    "\n",
    "- TF-IDF \n",
    "\n",
    "- Averaging word2vec vectors\n",
    "\n",
    "- doc2vec\n",
    "\n",
    "- Deep IR \n",
    "\n",
    "- Word Mover's Distance\n",
    "\n",
    "The goal of this tutorial is to show the API so you can start tuning them yourself. Model tuning of the models is out of scope of this tutorial.\n",
    "\n",
    "We will also compare the accuracy of this first 'no tuning'/out of the box run of these techniques. It is in no way an indication of their best peformance that can be achieved with proper tuning. The benefit of the comparison is to manage the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "- Python 3\n",
    "- [Google News pre-trained word2vec (1.5 GB)](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n",
    "- gensim\n",
    "- sklearn\n",
    "- pandas\n",
    "- matplotlib\n",
    "- nltk with English stopwords\n",
    "- pyemd\n",
    "- 4 GB RAM\n",
    "- 8 GB disk space for WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "We will use MovieLens dataset linked with plots from OMDB. Thanks to [Sujit Pal](http://sujitpal.blogspot.de/2016/04/predicting-movie-tags-from-plots-using.html) for this linking idea. The prepared csv is in this repository. If you wish to link the datasets yourself - see the code in the [blog]((http://sujitpal.blogspot.de/2016/04/predicting-movie-tags-from-plots-using.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ihab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171156"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/tagged_plots_movielens.csv')\n",
    "df = df.dropna()\n",
    "df['plot'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The dataset is only __170k__ words. It is quite small but makes sure we don't have to wait a long time for the code to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_tags = ['sci-fi' , 'action', 'comedy', 'fantasy', 'animation', 'romance']\n",
    "df.tag.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data is very unbalanced. We have Comedy as majority class. \n",
    "\n",
    "A naive classifier that predicts everything to be comedy already achieves __40%__ accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The language in sci-fi plots differs a lot from action plots, so there should be some signal here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>movieId</th>\n",
       "      <th>plot</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A little boy named Andy loves to be in his roo...</td>\n",
       "      <td>animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>When two kids find and play a magical board ga...</td>\n",
       "      <td>fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Things don't seem to change much in Wabasha Co...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Hunters and their prey--Neil and his professio...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>An ugly duckling having undergone a remarkable...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>2443</td>\n",
       "      <td>148618</td>\n",
       "      <td>Three kids who travel back in time to 65 milli...</td>\n",
       "      <td>animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>2444</td>\n",
       "      <td>148624</td>\n",
       "      <td>Top Cat and the gang face a new police chief, ...</td>\n",
       "      <td>animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>2445</td>\n",
       "      <td>149088</td>\n",
       "      <td>Raggedy Ann and the rest of the toys in Marcel...</td>\n",
       "      <td>animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>2446</td>\n",
       "      <td>149406</td>\n",
       "      <td>Continuing his \"legendary adventures of awesom...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>2447</td>\n",
       "      <td>151451</td>\n",
       "      <td>A romance fantasy humorous situations cleverly...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2427 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  movieId                                               plot  \\\n",
       "0              0        1  A little boy named Andy loves to be in his roo...   \n",
       "1              1        2  When two kids find and play a magical board ga...   \n",
       "2              2        3  Things don't seem to change much in Wabasha Co...   \n",
       "3              3        6  Hunters and their prey--Neil and his professio...   \n",
       "4              4        7  An ugly duckling having undergone a remarkable...   \n",
       "...          ...      ...                                                ...   \n",
       "2443        2443   148618  Three kids who travel back in time to 65 milli...   \n",
       "2444        2444   148624  Top Cat and the gang face a new police chief, ...   \n",
       "2445        2445   149088  Raggedy Ann and the rest of the toys in Marcel...   \n",
       "2446        2446   149406  Continuing his \"legendary adventures of awesom...   \n",
       "2447        2447   151451  A romance fantasy humorous situations cleverly...   \n",
       "\n",
       "            tag  \n",
       "0     animation  \n",
       "1       fantasy  \n",
       "2        comedy  \n",
       "3        action  \n",
       "4       romance  \n",
       "...         ...  \n",
       "2443  animation  \n",
       "2444  animation  \n",
       "2445  animation  \n",
       "2446     comedy  \n",
       "2447    romance  \n",
       "\n",
       "[2427 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index][['plot', 'tag']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Genre:', example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a future world devastated by disease, a convict is sent back in time to gather information about the man-made virus that wiped out most of the human population on the planet.\n",
      "Genre: sci-fi\n"
     ]
    }
   ],
   "source": [
    "print_plot(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the popular video game of the same name \"Mortal Kombat\" tells the story of an ancient tournament where the best of the best of different Realms fight each other. The goal - ten wins to be able to legally invade the losing Realm. Outworld has so far collected nine wins against Earthrealm, so it's up to Lord Rayden and his fighters to stop Outworld from reaching the final victory...\n",
      "Genre: action\n"
     ]
    }
   ],
   "source": [
    "print_plot(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train/test split of 90/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data.tag.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model evaluation approach\n",
    "We will use confusion matrices to evaluate all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(my_tags))\n",
    "    target_names = my_tags\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    print('accuracy %s' % accuracy_score(target, predictions))\n",
    "    cm = confusion_matrix(target, predictions)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def predict(vectorizer, classifier, data):\n",
    "    data_features = vectorizer.transform(data['plot'])\n",
    "    predictions = classifier.predict(data_features)\n",
    "    target = data['tag']\n",
    "    evaluate_prediction(predictions, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baseline: bag of words, n-grams, tf-idf\n",
    "Let's start with some simple baselines before diving into more advanced methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest document feature is just a count of each word occurrence in a document.\n",
    "\n",
    "We remove stop-words and use NLTK tokenizer then limit our vocabulary to 3k most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2184x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 54806 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# training\n",
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english', max_features=3000) \n",
    "\n",
    "train_data_features = count_vectorizer.fit_transform(train_data['plot'])\n",
    "train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-modal logistic regression is a simple white-box classifier. We will use either logistic regression or KNN throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1417\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    762\u001b[0m             n_iter_i = _check_optimize_result(\n\u001b[0;32m    763\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n\u001b[0m\u001b[0;32m    765\u001b[0m             \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py\u001b[0m in \u001b[0;36m_check_optimize_result\u001b[1;34m(solver, result, max_iter, extra_warning_msg)\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[1;34m\"    https://scikit-learn.org/stable/modules/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[1;34m\"preprocessing.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             ).format(solver, result.status, result.message.decode(\"latin1\"))\n\u001b[0m\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_warning_msg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0mwarning_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra_warning_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_data_features, train_data['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advanced',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'advice',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'afraid']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[80:90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nothing impressive - only 2% better better than the classifier that thinks that everything is a comedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-3acd160143cc>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(vectorizer, classifier, data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdata_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'plot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mevaluate_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predict(count_vectorizer, logreg, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "White box vectorizer and classifier are great! We can see what are the most important words for sci-fi. This makes it very easy to tune and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_influential_words(vectorizer, genre_index=0, num_words=10):\n",
    "    features = vectorizer.get_feature_names()\n",
    "    max_coef = sorted(enumerate(logreg.coef_[genre_index]), key=lambda x:x[1], reverse=True)\n",
    "    return [features[x[0]] for x in max_coef[:num_words]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2646d4004f23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgenre_tag_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_tags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgenre_tag_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmost_influential_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenre_tag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-c5545977677c>\u001b[0m in \u001b[0;36mmost_influential_words\u001b[1;34m(vectorizer, genre_index, num_words)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmost_influential_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenre_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmax_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgenre_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmax_coef\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# words for the fantasy genre\n",
    "genre_tag_id = 1\n",
    "print(my_tags[genre_tag_id])\n",
    "most_influential_words(count_vectorizer, genre_tag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_data_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Character N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A character _n-gram_ is a chunk of a document of length _n_. It is a poor man's tokenizer but sometimes works well. The parameter _n_ depends on language and the corpus. We choose length between 3 and 6 characters and to only focus on 3k most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_gram_vectorizer = CountVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=([2,5]),\n",
    "    tokenizer=None,    \n",
    "    preprocessor=None,                               \n",
    "    max_features=3000) \n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "train_data_features = n_gram_vectorizer.fit_transform(train_data['plot'])\n",
    "\n",
    "logreg = logreg.fit(train_data_features, train_data['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_gram_vectorizer.get_feature_names()[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The results are worse than using a tokenizer and bag of words. Probably due to not removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(n_gram_vectorizer, logreg, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "[Term Frequency - Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a little more advanced way to count words in a document.\n",
    "It adjusts for document length, word frequency and most importantly for frequency of a particular word in a particular document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf_vect = TfidfVectorizer(\n",
    "    min_df=2, tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english')\n",
    "train_data_features = tf_vect.fit_transform(train_data['plot'])\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_data_features, train_data['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tf_vect.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predict(tf_vect, logreg, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "White box vectorizer and classifier are great! We can see what are the most important words for sci-fi. This makes it very easy to tune and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_influential_words(tf_vect, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Things to try with bag of words\n",
    "\n",
    "10 mins for exercises.\n",
    "\n",
    "For more insight into the model print out the most influential words for a particular plot.\n",
    "\n",
    "Try n-grams with TF-IDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Averaging word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use more complex features rather than just counting words.\n",
    "\n",
    "A great recent achievement of NLP is the [word2vec embedding](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). See Chris Moody's [video](https://www.youtube.com/watch?v=vkfXBGnDplQ) for a great introduction to word2vec. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First we load a word2vec model. It has been pre-trained by Google on a 100 billon word Google News corpus. You can play with this model using a fun [web-app](http://rare-technologies.com/word2vec-tutorial/#app).\n",
    "\n",
    "Link to the web-app: http://rare-technologies.com/word2vec-tutorial/#app\n",
    "\n",
    "Vocabulary size: 3 mln words. \n",
    "\n",
    "__Warning__: 3 mins to load, takes 4 GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os\n",
    "glove2word2vec(glove_input_file=\"data\\\\vectors.50.txt\", word2vec_output_file=\"data\\gensim_glove_vectors.txt\")\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format( \"data\\gensim_glove_vectors.txt\", binary=False)\n",
    "os.remove('data\\gensim_glove_vectors.txt')\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13000, 13020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have a vector for each word. How do we get a vector for a sequence of words (aka a document)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most naive way is just to take an average. [Mike Tamir](https://www.youtube.com/watch?v=7gTjYwiaJiU) has suggested that the resulting vector points to a single word summarising the whole document. For example all words in a book\n",
    " ‘A tale of two cities’ should add up to 'class-struggle’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/naivedoc2vec.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.layer1_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For word2vec we apply a different tokenization. We want to preserve case as the vocabulary distingushes lower and upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how KNN and logistic regression classifiers perform on these word-averaging document features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_naive_dv = KNeighborsClassifier(n_neighbors=3, n_jobs=1, algorithm='brute', metric='cosine' )\n",
    "knn_naive_dv.fit(X_train_word_average, train_data.tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predicted = knn_naive_dv.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(predicted, test_data.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "KNN is even worse than the naive 'everything is comedy' baseline! Let's see if logistic regression is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "logreg = logreg.fit(X_train_word_average, train_data['tag'])\n",
    "predicted = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great! It gives __54%__ accuracy. Best that we have seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(predicted, test_data.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now just for fun let's see if text summarisation works on our data. Let's pick a plot and see which words it averages to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.iloc()[56]['plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hmm... The summarisation doesn't work here. Any ideas why? Hint: look at the area where the average ends up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=[X_test_word_average[56]], restrict_vocab=100000, topn=30)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec things to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "10 mins exercise\n",
    "\n",
    "Remove stop-words. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if word in stopwords.words('english'):\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What accuracy do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### More word2vec things to try\n",
    "\n",
    "Experiment with other pre-trained models - see nice [list](https://github.com/3Top/word2vec-api/) from 3Top.\n",
    "\n",
    "\n",
    "Use Gensim's GloVe converter.\n",
    "\n",
    "\n",
    "Do IDF weighting in the averaging function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) by Google suggests a model for document classification called Paragraph Vectors Doc2Vec or Doc2vec in short. It is very similar to word2vec. \n",
    "\n",
    "It introduces 'a tag' - a word that is in every context in the document.\n",
    "\n",
    "For our first try we tag every plot with its genre. This makes it 'semi-supervised' learning - the genre labels is just one objective among many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['plot']), tags=[r.tag]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagged = test_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['plot']), tags=[r.tag]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a training entry looks like - an example plot tagged by 'sci-fi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagged.values[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainsent = train_tagged.values\n",
    "testsent = test_tagged.values\n",
    "\n",
    "# simple gensim doc2vec api\n",
    "doc2vec_model = Doc2Vec(trainsent, workers=1, size=5, iter=20, dm=1)\n",
    "\n",
    "train_targets, train_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in trainsent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interesting thing about doc2vec is that we need to run gradient descent during prediction to infer the vector for an unseen document. An unseen document is initially assigned a random vector and then this vector fit by gradient descent. Because of this randomness we get different vectors on re-runs of the next cell.\n",
    "\n",
    "Consequently, the accuracy of logistic regression changes when the test set vectors change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_targets, test_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in testsent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_regressors, train_targets)\n",
    "evaluate_prediction(logreg.predict(test_regressors), test_targets, title=str(doc2vec_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "KNN gives a lower accuracy than logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "knn_test_predictions = [\n",
    "    doc2vec_model.docvecs.most_similar([pred_vec], topn=1)[0][0]\n",
    "    for pred_vec in test_regressors\n",
    "]\n",
    "evaluate_prediction(knn_test_predictions, test_targets, str(doc2vec_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doc2vec gives us a vector for each genre so we can see which genres are close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs.most_similar('action')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Words surrounding the 'sci-fi' tag describe it pretty accurately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.most_similar([doc2vec_model.docvecs['sci-fi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Doc2vec exercise\n",
    "\n",
    "10 mins\n",
    "\n",
    "Find the random seed that gives the best prediction. :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 20\n",
    "\n",
    "doc2vec_model.seed = seed\n",
    "doc2vec_model.random = random.RandomState(seed)\n",
    "\n",
    "\n",
    "test_targets, test_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in testsent])\n",
    "\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5, random_state=42)\n",
    "logreg = logreg.fit(train_regressors, train_targets)\n",
    "evaluate_prediction(logreg.predict(test_regressors), test_targets, title=str(doc2vec_model))\n",
    "print(doc2vec_model.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Doc2vec things to try\n",
    "Try tagging every sentence with a unique tag 'SENT_123' and then apply KNN. \n",
    "\n",
    "Try multiple tags per plot as in this repo published __today__ : https://github.com/sindbach/doc2vec_pymongo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Deep IR' is a technique developed by  [“Document Classification by Inversion of Distributed Language Representations”, Matt Taddy](http://arxiv.org/pdf/1504.07295v3.pdf). Matt has contributed a gensim [tutorial](https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb) - great source of more in depth information.\n",
    "\n",
    "In short the algorithm is:\n",
    "\n",
    "1. Train a word2vec model only on comedy plots.\n",
    "\n",
    "2. Trian another model only on sci-fi, another on romance etc. Get 6 models - one for each genre.\n",
    "\n",
    "3. Take a plot and see which model fits it best using Bayes' Theorem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The tokenization is different from other methods. The reason for this is that we are following an original approach in the paper. The purpose of this tutorial is to see how the models behave out of the box.\n",
    "\n",
    "We just clean non-alphanumeric characters and split by sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "contractions = re.compile(r\"'|-|\\\"\")\n",
    "# all non alphanumeric\n",
    "symbols = re.compile(r'(\\W+)', re.U)\n",
    "# single character removal\n",
    "singles = re.compile(r'(\\s\\S\\s)', re.I|re.U)\n",
    "# separators (any whitespace)\n",
    "seps = re.compile(r'\\s+')\n",
    "\n",
    "# cleaner (order matters)\n",
    "def clean(text): \n",
    "    text = text.lower()\n",
    "    text = contractions.sub('', text)\n",
    "    text = symbols.sub(r' \\1 ', text)\n",
    "    text = singles.sub(' ', text)\n",
    "    text = seps.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "# sentence splitter\n",
    "alteos = re.compile(r'([!\\?])')\n",
    "def sentences(l):\n",
    "    l = alteos.sub(r' \\1 .', l).rstrip(\"(\\.)*\\n\")\n",
    "    return l.split(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plots(label):\n",
    "    my_df = None\n",
    "    if label=='training':\n",
    "        my_df = train_data\n",
    "    else:\n",
    "        my_df = test_data\n",
    "    for i, row in my_df.iterrows():\n",
    "        yield {'y':row['tag'],\\\n",
    "        'x':[clean(s).split() for s in sentences(row['plot'])]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The corpus is small so can be read into memory\n",
    "revtrain = list(plots(\"training\"))\n",
    "revtest = list(plots(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training set for unbiased word2vec training\n",
    "np.random.shuffle(revtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def tag_sentences(reviews, stars=my_tags):  \n",
    "    for r in reviews:\n",
    "        if r['y'] in stars:\n",
    "            for s in r['x']:\n",
    "                yield s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example `sci-fi` sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(tag_sentences(revtrain, my_tags[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We train our own 6 word2vec models from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "## training\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=100, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0, # we only have scoring for the hierarchical softmax setup\n",
    "    \n",
    "    )\n",
    "print(basemodel)\n",
    "basemodel.build_vocab(tag_sentences(revtrain)) \n",
    "from copy import deepcopy\n",
    "genremodels = [deepcopy(basemodel) for i in range(len(my_tags))]\n",
    "for i in range(len(my_tags)):\n",
    "    slist = list(tag_sentences(revtrain, my_tags[i]))\n",
    "    print(my_tags[i], \"genre (\", len(slist), \")\")\n",
    "    genremodels[i].train( slist, total_examples=len(slist) ,epochs = basemodel.epochs)\n",
    "# get the probs (note we give docprob a list of lists of words, plus the models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will compute most likely class for a plot using Bayes' Theorem formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/deep_ir_bayes.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For any new sentence we can obtain its _likelihood_ (lhd; actually, the composite likelihood approximation; see the paper) using the [score](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.score) function in the `word2vec` class.  We get the likelihood for each sentence in the first test review, then convert to a probability over star ratings. Every sentence in the review is evaluated separately and the final star rating of the review is an average vote of all the sentences. This is all in the following handy wrapper. (from the original [tutorial](https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb) by Matt Taddy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## predict\n",
    "probs = docprob( [r['x'] for r in revtest], genremodels )  \n",
    "predictions = probs.idxmax(axis=1).apply(lambda x: my_tags[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tag_index = 0\n",
    "col_name = \"out-of-sample prob positive for \" + my_tags[tag_index]\n",
    "probpos = pd.DataFrame({col_name:probs[[tag_index]].sum(axis=1), \n",
    "                        \"true genres\": [r['y'] for r in revtest]})\n",
    "probpos.boxplot(col_name,by=\"true genres\", figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "target = [r['y'] for r in revtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prediction(predictions, target, \"Deep IR with word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Performance is worse than for a naive predictor that says that everything is `comedy`.\n",
    "\n",
    "### Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "It is because we train each word2vec model from scratch on a very small sample of about 30k words.\n",
    "\n",
    "This model needs more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we shown how to run 'hello-world' in 7 different document classification techniques. It is just a beginning of exploration of their features... There are a lot of parameters that can be tuned to get the best possible results out of them. The 'hello-world' run is in no way an indication of their best peformance. The goal of this tutorial is to show the API so you can start tuning them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box \"no tuning\" accuracy of bag of words is not far behind more advanced techniques. \n",
    "Tune them and the pre-processing for them well first and only then reach for more advanced methods if more accuracy is absolutely needed."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
